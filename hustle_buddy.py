from fastapi import FastAPI, HTTPException, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from pydantic import BaseModel, validator
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from dotenv import load_dotenv
import uvicorn
import json

load_dotenv()

app = FastAPI(title="Hustle Buddy API", description="AI Model Response Evaluation API")

model = OpenAIChat(id="gpt-4o-mini")

class ModelComparisonRequest(BaseModel):
    prompt: str
    model_1: str
    model_2: str
    model_3: str
    
    @validator('prompt', 'model_1', 'model_2', 'model_3')
    def validate_non_empty(cls, v):
        if not v or not v.strip():
            raise ValueError('Field cannot be empty')
        return v

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    return JSONResponse(
        status_code=422,
        content={
            "error": "Invalid request format",
            "details": str(exc),
            "tip": "Make sure your JSON is properly formatted. Escape quotes with \\ and newlines with \\n"
        }
    )

tasks = []

def add_task(task_description):
    tasks.append({"description": task_description, "completed": False})
    return f"Task added: {task_description}"

def list_tasks():
    if not tasks:
        return "No tasks found."
    task_list = "\n".join([f"{i+1}. {task['description']} [{'x' if task['completed'] else ' '}]" for i, task in enumerate(tasks)])
    return f"Current tasks:\n{task_list}"

def complete_task(task_index):
    if 0 <= task_index < len(tasks):
        tasks[task_index]["completed"] = True
        return f"Task {task_index + 1} marked as completed."
    return "Invalid task index."

def productivity_tip():
    tips = [
        "Break tasks into smaller steps to avoid feeling overwhelmed.",
        "Use the Pomodoro technique: Work for 25 minutes, then take a 5-minute break.",
        "Prioritize tasks using the Eisenhower Matrix (Urgent vs. Important)."
    ]
    import random
    return random.choice(tips)

hustle_buddy = Agent(
    name="HustleBuddy",
    model=model,
    description="""
    You are developing a system that evaluates the quality of responses generated by multiple AI models. Given a specific input prompt, you collect outputs from Model 1, Model 2, and Model 3.

    Your goal is to use GPT to analyze these responses and determine:

    Which aspects (rubrics) Model 1 performs better than the other models.

    Which aspects Model 1 performs worse than at least one of the others.

    Each rubric should be labeled as either:

    Critical rubric: A core, essential aspect directly related to fulfilling the intent and requirements of the prompt.

    Non-critical rubric: A secondary, "nice-to-have" feature such as formatting, extra clarity, or conciseness that improves quality but isn't required.""",
    instructions=[
        """
        You will be given the following inputs:
        
        - A prompt that was used to query the models.
        - The response from Model 1.
        - The response from Model 2.
        - The response from Model 3.
        """,
        """
        Your goal is to analyze these responses and determine:

        - Compare Model 1 against Model 2 and Model 3 across specific, meaningful criteria.
        - Identify at least 10 rubrics where Model 1 performs better than the others.
        - Identify rubrics where Model 1 performs worse than at least one other model.

        For each rubric, classify it as either:
        - Critical: If it directly affects the correctness, completeness, or alignment with the prompt.
        - Non-critical: If it's a helpful but non-essential quality.
        """
    ],
    expected_output=
    """
    ✅ Rubrics where Model 1 performs better
    - Each rubric should include a short title, classification (Critical or Non-critical), and an explanation.

    [Rubric Name] - [Critical / Non-critical]
    Explanation of why Model 1 outperforms the others in this aspect.

    [Rubric Name] - [Critical / Non-critical]
    Explanation of why Model 1 outperforms the others in this aspect.

    ...

    ⚠️ Rubrics where Model 1 performs worse
    - Each rubric should include a short title, classification (Critical or Non-critical), and an explanation.

    [Rubric Name] - [Critical / Non-critical]
    Explanation of what Model 1 is missing or doing poorly, and how the other model(s) handle it better.

    [Rubric Name] - [Critical / Non-critical]
    Explanation of what Model 1 is missing or doing poorly, and how the other model(s) handle it better.

    ...
    """,
    # tools=[
    #     add_task,
    #     list_tasks,
    #     complete_task,
    #     productivity_tip
    # ],
    show_tool_calls=True,
    markdown=True
)

@app.get("/")
async def root():
    return {
        "message": "Hustle Buddy API is running! Use POST /evaluate to analyze AI model responses.",
        "endpoints": {
            "evaluate": "POST /evaluate - Compare AI model responses",
            "docs": "GET /docs - Interactive API documentation",
            "health": "GET /health - Health check"
        }
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "Hustle Buddy API"}

@app.post("/evaluate")
async def evaluate_models(request: ModelComparisonRequest):
    """
    Evaluate and compare responses from three AI models.
    
    Args:
        request: JSON with prompt, model_1, model_2, model_3 responses
        
    Returns:
        Analysis comparing Model 1 against Models 2 and 3
    """
    try:
        # Validate input lengths (optional safeguard)
        if len(request.prompt) > 10000:
            raise HTTPException(status_code=400, detail="Prompt too long (max 10,000 characters)")
        
        for field_name, field_value in [("model_1", request.model_1), ("model_2", request.model_2), ("model_3", request.model_3)]:
            if len(field_value) > 50000:
                raise HTTPException(status_code=400, detail=f"{field_name} response too long (max 50,000 characters)")
        
        # Construct the evaluation prompt
        evaluation_prompt = f"""
        Please analyze and compare the following AI model responses:

        **Original Prompt:**
        {request.prompt}

        **Model 1 Response:**
        {request.model_1}

        **Model 2 Response:**
        {request.model_2}

        **Model 3 Response:**
        {request.model_3}

        Please evaluate Model 1 against Models 2 and 3 according to your instructions.
        """
        
        # Get the analysis from hustle_buddy
        response = hustle_buddy.run(evaluation_prompt)
        
        return {
            "status": "success",
            "analysis": response.content,
            "metadata": {
                "prompt_length": len(request.prompt),
                "model_1_length": len(request.model_1),
                "model_2_length": len(request.model_2),
                "model_3_length": len(request.model_3)
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error during evaluation: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)