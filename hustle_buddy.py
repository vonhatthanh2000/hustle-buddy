from fastapi import FastAPI, HTTPException, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from pydantic import BaseModel, validator
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf import PDFKnowledge
from agno.vectordb.lancedb import LanceDb, SearchType
from agno.storage.sqlite import SqliteStorage
from knowledge_rag import knowledge_base
from dotenv import load_dotenv
import uvicorn
import json
import os
from pathlib import Path

load_dotenv()

app = FastAPI(title="Hustle Buddy API", description="AI Model Response Evaluation API with Knowledge Base")

model = OpenAIChat(id="gpt-4o-mini")

# Setup knowledge base from docs folder
docs_folder = Path("docs")
pdf_files = list(docs_folder.glob("*.pdf"))

knowledge = None
storage = None

if pdf_files:
    print(f"üìö Loading knowledge base from {len(pdf_files)} PDF files...")
    knowledge = PDFKnowledge(
        path=docs_folder,
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="hustle_buddy_docs",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small", dimensions=1536),
        ),
    )

# Setup storage for agent sessions
storage = SqliteStorage(table_name="agent_sessions", db_file="tmp/agent_sessions.db")

class ModelComparisonRequest(BaseModel):
    prompt: str
    model_1: str
    model_2: str
    model_3: str
    use_knowledge: bool = False
    
    @validator('prompt', 'model_1', 'model_2', 'model_3')
    def validate_non_empty(cls, v):
        if not v or not v.strip():
            raise ValueError('Field cannot be empty')
        return v

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    return JSONResponse(
        status_code=422,
        content={
            "error": "Invalid request format",
            "details": str(exc),
            "tip": "Make sure your JSON is properly formatted. Escape quotes with \\ and newlines with \\n"
        }
    )

tasks = []

def add_task(task_description):
    tasks.append({"description": task_description, "completed": False})
    return f"Task added: {task_description}"

def list_tasks():
    if not tasks:
        return "No tasks found."
    task_list = "\n".join([f"{i+1}. {task['description']} [{'x' if task['completed'] else ' '}]" for i, task in enumerate(tasks)])
    return f"Current tasks:\n{task_list}"

def complete_task(task_index):
    if 0 <= task_index < len(tasks):
        tasks[task_index]["completed"] = True
        return f"Task {task_index + 1} marked as completed."
    return "Invalid task index."

def productivity_tip():
    tips = [
        "Break tasks into smaller steps to avoid feeling overwhelmed.",
        "Use the Pomodoro technique: Work for 25 minutes, then take a 5-minute break.",
        "Prioritize tasks using the Eisenhower Matrix (Urgent vs. Important)."
    ]
    import random
    return random.choice(tips)

hustle_buddy = Agent(
    name="HustleBuddy",
    model=model,
    description="""
    You are developing a system that evaluates the quality of responses generated by multiple AI models. Given a specific input prompt, you collect outputs from Model 1, Model 2, and Model 3.

    Your goal is to use GPT to analyze these responses and determine:

    Which aspects (rubrics) Model 1 performs better than the other models.

    Which aspects Model 1 performs worse than at least one of the others.

    Each rubric should be labeled as either:

    Critical rubric: A core, essential aspect directly related to fulfilling the intent and requirements of the prompt.

    Non-critical rubric: A secondary, "nice-to-have" feature such as formatting, extra clarity, or conciseness that improves quality but isn't required.""",
    instructions=[
        """
        You will be given the following inputs:
        
        - A prompt that was used to query the models.
        - The response from Model 1.
        - The response from Model 2.
        - The response from Model 3.
        """,
        """
        Your goal is to analyze these responses and determine:

        - Compare Model 1 against Model 2 and Model 3 across specific, meaningful criteria.
        - Identify at least 10 rubrics where Model 1 performs better than the others.
        - Identify rubrics where Model 1 performs worse than at least one other model.

        For each rubric, classify it as either:
        - Critical: If it directly affects the correctness, completeness, or alignment with the prompt.
        - Non-critical: If it's a helpful but non-essential quality.
        """,
        "If you have access to knowledge base, search it for relevant information before providing analysis.",
        "Use knowledge from documentation to inform your evaluation criteria when available."
    ],
    expected_output=
    """
    ‚úÖ Rubrics where Model 1 performs better
    - Each rubric should include a short title, classification (Critical or Non-critical), and an explanation.

    [Rubric Name] - [Critical / Non-critical]
    Explanation of why Model 1 outperforms the others in this aspect.

    [Rubric Name] - [Critical / Non-critical]
    Explanation of why Model 1 outperforms the others in this aspect.

    ...

    ‚ö†Ô∏è Rubrics where Model 1 performs worse
    - Each rubric should include a short title, classification (Critical or Non-critical), and an explanation.

    [Rubric Name] - [Critical / Non-critical]
    Explanation of what Model 1 is missing or doing poorly, and how the other model(s) handle it better.

    [Rubric Name] - [Critical / Non-critical]
    Explanation of what Model 1 is missing or doing poorly, and how the other model(s) handle it better.

    ...
    """,
    knowledge=knowledge_base,
    storage=storage,
    add_datetime_to_instructions=True,
    add_history_to_messages=True,
    num_history_runs=3,
    show_tool_calls=True,
    markdown=True
)

@app.get("/")
async def root():
    return {
        "message": "Hustle Buddy API is running! Use POST /evaluate to analyze AI model responses.",
        "endpoints": {
            "evaluate": "POST /evaluate - Compare AI model responses",
            "knowledge-status": "GET /knowledge-status - Check knowledge base status",
            "load-knowledge": "POST /load-knowledge - Load/reload knowledge base",
            "docs": "GET /docs - Interactive API documentation",
            "health": "GET /health - Health check"
        },
        "features": {
            "knowledge_base": knowledge is not None,
            "storage": storage is not None,
            "pdf_files_found": len(pdf_files) if pdf_files else 0
        }
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "Hustle Buddy API"}

@app.get("/knowledge-status")
async def knowledge_status():
    """Check the status of the knowledge base."""
    if not knowledge:
        return {"status": "no_knowledge", "message": "No PDF files found in docs folder"}
    
    try:
        # Check if knowledge is loaded
        vector_db_exists = knowledge.vector_db.table_exists()
        return {
            "status": "ready" if vector_db_exists else "not_loaded",
            "knowledge_available": knowledge is not None,
            "vector_db_ready": vector_db_exists,
            "pdf_files": [str(f.name) for f in pdf_files]
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/load-knowledge")
async def load_knowledge(recreate: bool = False):
    """Load or reload the knowledge base from PDF files."""
    if not knowledge:
        raise HTTPException(status_code=400, detail="No PDF files found in docs folder")
    
    try:
        print(f"üìö Loading knowledge base (recreate={recreate})...")
        knowledge.load(recreate=recreate)
        return {
            "status": "success",
            "message": f"Knowledge base loaded from {len(pdf_files)} PDF files",
            "pdf_files": [str(f.name) for f in pdf_files]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading knowledge base: {str(e)}")

@app.post("/evaluate")
async def evaluate_models(request: ModelComparisonRequest):
    """
    Evaluate and compare responses from three AI models.
    
    Args:
        request: JSON with prompt, model_1, model_2, model_3 responses
        use_knowledge: Whether to use knowledge base for evaluation
        
    Returns:
        Analysis comparing Model 1 against Models 2 and 3
    """
    try:
        # Validate input lengths (optional safeguard)
        if len(request.prompt) > 10000:
            raise HTTPException(status_code=400, detail="Prompt too long (max 10,000 characters)")
        
        for field_name, field_value in [("model_1", request.model_1), ("model_2", request.model_2), ("model_3", request.model_3)]:
            if len(field_value) > 50000:
                raise HTTPException(status_code=400, detail=f"{field_name} response too long (max 50,000 characters)")
        
        # Construct the evaluation prompt
        evaluation_prompt = f"""
        Please analyze and compare the following AI model responses:

        **Original Prompt:**
        {request.prompt}

        **Model 1 Response:**
        {request.model_1}

        **Model 2 Response:**
        {request.model_2}

        **Model 3 Response:**
        {request.model_3}

        Please evaluate Model 1 against Models 2 and 3 according to your instructions.
        """
        
        if request.use_knowledge and knowledge:
            evaluation_prompt += "\n\nPlease search your knowledge base for relevant evaluation criteria and best practices before providing your analysis."
        
        # Get the analysis from hustle_buddy
        response = hustle_buddy.run(evaluation_prompt)
        
        return {
            "status": "success",
            "analysis": response.content,
            "metadata": {
                "prompt_length": len(request.prompt),
                "model_1_length": len(request.model_1),
                "model_2_length": len(request.model_2),
                "model_3_length": len(request.model_3),
                "knowledge_used": request.use_knowledge and knowledge is not None,
                "session_id": response.session_id if hasattr(response, 'session_id') else None
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error during evaluation: {str(e)}")

if __name__ == "__main__":
    # Load knowledge base on startup if available
    if knowledge:
        try:
            print("üöÄ Loading knowledge base on startup...")
            knowledge.load(recreate=False)
            print("‚úÖ Knowledge base loaded successfully!")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not load knowledge base: {e}")
            print("üí° You can load it manually using POST /load-knowledge")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)